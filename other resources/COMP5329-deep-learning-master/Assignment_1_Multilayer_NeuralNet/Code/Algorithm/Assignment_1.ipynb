{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Import data'''\n",
    "\n",
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "'''PLEASE REPLACE THE LOCATION OF THE FOLLOWING 3 INPUT FILES ACCORDINLY '''\n",
    "\n",
    "with h5py.File(r\"C:\\Users\\nqtru\\Desktop\\COMP5329\\Assignment 1\\Assignment-1-Dataset\\train_128.h5\",'r') as TU: \n",
    "    data = np.copy(TU['data'])\n",
    "with h5py.File(r\"C:\\Users\\nqtru\\Desktop\\COMP5329\\Assignment 1\\Assignment-1-Dataset\\train_label.h5\",'r') as TL:\n",
    "    label = np.copy(TL['label'])\n",
    "# Import predicted outputs for test data from Predicted_labels.h5\n",
    "with h5py.File(r\"C:\\Users\\nqtru\\Desktop\\COMP5329\\Assignment 1\\470518197_470490653_308012798\\Code\\Output\\Predicted_labels.h5\",'r') as TB:\n",
    "    predicted_label = np.copy(TB['label'])\n",
    "    \n",
    "'''Standardise the data'''\n",
    "\n",
    "mu = data.mean()\n",
    "sigma = data.std()\n",
    "data = (data - mu) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define class for activation functions'''\n",
    "\n",
    "class Activation(object):\n",
    "    \n",
    "    def relu (self,x):\n",
    "        return np.maximum(x,0)  \n",
    "\n",
    "    def relu_deriv (self,a):\n",
    "        # a = np.maximum(x,0)\n",
    "        a[a<0] = 0\n",
    "        a[a>=0] = 1\n",
    "        return a\n",
    "    \n",
    "    def leaky_relu (self,x):\n",
    "        return np.maximum(x,0.1*x)  \n",
    "\n",
    "    def leaky_relu_deriv (self,a):\n",
    "        # a = np.maximum(x,0.1*x)\n",
    "        a[a<0] = 0.1\n",
    "        a[a>=0] = 1\n",
    "        return a\n",
    "    \n",
    "    def softmax (self,x):\n",
    "        # Normalise the input to prevent overflow problem in np.exp\n",
    "        x_max = x.max()\n",
    "        x_norm = x - x_max\n",
    "        return np.exp(x_norm) / np.sum(np.exp(x_norm), axis=0)         \n",
    "   \n",
    "    def softmax_deriv (self, a):\n",
    "        #a = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return a * (1 - a )\n",
    "        \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    \n",
    "    def logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a )      \n",
    "            \n",
    "    def __init__(self,activation='relu'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.logistic\n",
    "            self.f_deriv = self.logistic_deriv\n",
    "        elif activation == 'softmax':\n",
    "            self.f = self.softmax\n",
    "            self.f_deriv = self.softmax_deriv          \n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.tanh\n",
    "            self.f_deriv = self.tanh_deriv\n",
    "        elif activation == 'relu':\n",
    "            self.f = self.relu\n",
    "            self.f_deriv = self.relu_deriv\n",
    "        elif activation == 'leaky relu':\n",
    "            self.f = self.leaky_relu\n",
    "            self.f_deriv = self.leaky_relu_deriv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define class for one hidden layer'''\n",
    "\n",
    "class HiddenLayer(object): \n",
    "    \n",
    "    # To keep the initialised weights and biases stable in across epochs\n",
    "    import random\n",
    "    random.seed(1)\n",
    "    \n",
    "    # Initialisation\n",
    "    def __init__(self,n_in, n_out, W=None, b=None, activation=[0,1]):\n",
    "        \"\"\"   \n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: list of string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation[0]).f # Current layer's activation function\n",
    "        self.activation_deriv=Activation(activation[1]).f_deriv # Previous layer's derivative activation function\n",
    "        \n",
    "        '''\n",
    "        Initialize W\n",
    "        Weight matrix W is of shape (n_in,n_out)\n",
    "        '''     \n",
    "        # Uniformly sampled with a variance of 2/n_in (He, et al, 2015)\n",
    "        if activation[0] == 'relu':\n",
    "            self.W = np.random.uniform(\n",
    "                    low=-np.sqrt(2. / (n_in)), \n",
    "                    high=np.sqrt(2. / (n_in)),\n",
    "                    size=(n_in, n_out)\n",
    "            )\n",
    "        # Uniformly sampled with a variance of 6/(n_in+n_out) (Glorot & Bengio, 2010)\n",
    "        else: \n",
    "            self.W = np.random.uniform(\n",
    "                    low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                    high=np.sqrt(6. / (n_in + n_out)),\n",
    "                    size=(n_in, n_out)\n",
    "            )\n",
    "        # 4 times larger weights for logistic activation (Glorot & Bengio, 2010)\n",
    "        if activation[0] == 'logistic':\n",
    "            self.W *= 4\n",
    "            \n",
    "        '''\n",
    "        Initialize b\n",
    "        The bias vector b is of shape (n_out,)\n",
    "        '''\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        ''' Intialise gradients of W,b of same shape as W,b'''\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        ''' Intialise velocities of W,b for Momentum & Adam updates, same shape as W,b'''\n",
    "        self.velocity_W = np.zeros(self.W.shape)\n",
    "        self.velocity_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        ''' Intialise squared gradients for Adam update, same shape as W,b '''\n",
    "        self.sqr_grad_W = np.zeros(self.W.shape) \n",
    "        self.sqr_grad_b = np.zeros(self.b.shape) \n",
    "        \n",
    "        \n",
    "    '''Forward Propagation'''\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :input: input data/activations from previous layers\n",
    "        '''\n",
    "        #hidden_layer_input= matrix_dot_product(X,wh) + bh\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "        return self.output\n",
    "    \n",
    "    '''Backward Propagation'''\n",
    "    def backward(self, delta, learning_rate):\n",
    "        # Calculate gradients of W,b\n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = delta\n",
    "        # return delta_ for next layer\n",
    "        delta_ = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta_\n",
    "    \n",
    "    '''Update parameters with momentum'''\n",
    "    def momentum_update(self, learning_rate=0.001, beta=0.9):\n",
    "        # Update velocities of W,b\n",
    "        self.velocity_W = beta*self.velocity_W + (1-beta)*self.grad_W\n",
    "        self.velocity_b = beta*self.velocity_b + (1-beta)*self.grad_b\n",
    "        # Update W,b\n",
    "        self.W -= learning_rate * self.velocity_W\n",
    "        self.b -= learning_rate * self.velocity_b \n",
    "    \n",
    "    '''Update parameters with Adam'''\n",
    "    ### Reference: Andrew Ng's Deep Learning on Coursera\n",
    "    ### https://www.coursera.org/specializations/deep-learning\n",
    "    def adam_update(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        '''\n",
    "        :beta1, beta2: values of beta (float) for Adam update\n",
    "        :epsilon: a very small float\n",
    "        '''\n",
    "        # Update velocity of W,b\n",
    "        self.velocity_W = beta1*self.velocity_W + (1-beta1)*self.grad_W\n",
    "        self.velocity_b = beta1*self.velocity_b + (1-beta1)*self.grad_b\n",
    "        # Correct the velocity of W,b\n",
    "        velocity_corrected_W = self.velocity_W / (1-beta1**2+epsilon)  # added epsilon to prevent 0 denominator\n",
    "        velocity_corrected_b = self.velocity_b / (1-beta1**2+epsilon)  # added epsilon to prevent 0 denominator\n",
    "        # Calculate squared gradients of W,b\n",
    "        self.sqr_grad_W = beta2 * self.sqr_grad_W + (1-beta2) * np.power(self.grad_W,2)\n",
    "        self.sqr_grad_b = beta2 * self.sqr_grad_b + (1-beta2) * np.power(self.grad_b,2)\n",
    "        # Correct the squared gradients of W,b\n",
    "        sqr_grad_corrected_W = self.sqr_grad_W / (1-beta2**2+epsilon) # added epsilon to prevent 0 denominator\n",
    "        sqr_grad_corrected_b = self.sqr_grad_b / (1-beta2**2+epsilon) # added epsilon to prevent 0 denominator   \n",
    "        # Update W,b\n",
    "        self.W -= learning_rate * (velocity_corrected_W/(np.sqrt(sqr_grad_corrected_W)+epsilon)) # added epsilon to prevent 0 denominator\n",
    "        self.b -= learning_rate * (velocity_corrected_b/(np.sqrt(sqr_grad_corrected_b)+epsilon)) # added epsilon to prevent 0 denominator\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define class for the neural network'''\n",
    "\n",
    "class MLP:\n",
    "    \n",
    "    # Initialisation\n",
    "    def __init__(self, layers, activations):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activations: The list of activation functions to be used. Can be\n",
    "        \"logistic\", \"tanh\", \"relu\" , \"leaky relu\" or \"softmax\n",
    "        \"\"\"        \n",
    "        ### initialize layers\n",
    "        self.layers=[]  \n",
    "        self.activations = activations # activation functions of each layer, index corresponds layer number\n",
    "        self.output_count = layers[-1] # number of output in the output layer\n",
    "        self.params=[]\n",
    "        self.dropout_fraction = 1 # 1 means no dropout, otherwise would be between 0 and 1\n",
    "        \n",
    "        # Create hidden layers\n",
    "        for i in range(len(layers)-1):\n",
    "            # Create a new layer with params: number of inputs, outputs, activation of itself and of previous layer\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation=[activations[i],activations[i-1]]))   \n",
    "    \n",
    "    '''Forward propagation'''\n",
    "    def forward(self,input,predict):\n",
    "        \"\"\"\n",
    "        :type input: numpy.array\n",
    "        :input: input data/activations from previous layers\n",
    "        :type input: boolean\n",
    "        :predict: True means forward step in prediction, \n",
    "                  False means forward step in training \n",
    "        \"\"\"\n",
    "        for i in range(len(self.layers)):\n",
    "            output = self.layers[i].forward(input)\n",
    "            # Apply dropout between hidden layers only\n",
    "            if i > 0 and i < len(self.layers)-1 and not predict: # Dropout in training steps\n",
    "                output = self.dropout(output)\n",
    "            elif predict: # Dropout in predict step \n",
    "                output = output * self.dropout_fraction     \n",
    "            input=output\n",
    "        return output\n",
    "    \n",
    "    '''Dropout module'''\n",
    "    ### Reference: Andrew Ng's Deep Learning on Coursera\n",
    "    ### https://www.coursera.org/specializations/deep-learning\n",
    "    def dropout(self,input):\n",
    "        # Create a random dropout array with given dropout percentage\n",
    "        dropout_array = np.random.binomial(1, self.dropout_fraction, size=input.shape[0])\n",
    "        input = input * dropout_array\n",
    "        return input\n",
    "    \n",
    "    '''Mean Squared Error'''\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        # Convert y_hat into an array of size 10 consisting of 0,1 (probability of each class)\n",
    "        y_true = np.zeros(y_hat.shape)\n",
    "        y_true[y] = 1\n",
    "        activation_deriv=Activation(self.activations[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y_true-y_hat\n",
    "        loss=error**2\n",
    "        # write down the delta in the last layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        return loss,delta\n",
    "    \n",
    "    '''Cross Entropy'''\n",
    "    def cross_entropy_loss(self,y,y_hat):\n",
    "        # Convert y_hat into an array of size 10 consisting of 0,1 (probability of each class)\n",
    "        y_true = np.zeros(y_hat.shape)\n",
    "        y_true[y] = 1\n",
    "        # Cross entropy loss\n",
    "        loss = -np.log(y_hat[y]+1e-15) # adding 1e-15 to prevent log(0)\n",
    "        # write down the delta in the last layer\n",
    "        delta = y_hat-y_true \n",
    "        return loss,delta\n",
    "    \n",
    "    '''Backward propagation'''\n",
    "    def backward(self,delta,learning_rate=0.001):\n",
    "        for layer in reversed(self.layers):\n",
    "            delta = layer.backward(delta,learning_rate)\n",
    "            \n",
    "    '''Gradient Descent parameters W,b updates'''\n",
    "    def update(self,learning_rate=0.001,beta1=0.9,beta2=0.999,optimizer=None):\n",
    "        \"\"\"\n",
    "        :learning_rate: learning rate (float)\n",
    "        :beta1, beta2: beta values (float) for Momentum & Adam updates\n",
    "        :optimizer: Gradient Descent optimizer, can be \"momentum\", \"adam\" or None\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # Momentum\n",
    "            if optimizer == 'momentum':\n",
    "                layer.momentum_update(learning_rate,beta1)\n",
    "            # Adam\n",
    "            elif optimizer == 'adam':\n",
    "                layer.adam_update(learning_rate,beta1,beta2,1e-8)\n",
    "            # Stochastic only\n",
    "            elif optimizer == None:\n",
    "                layer.W -= learning_rate * layer.grad_W\n",
    "                layer.b -= learning_rate * layer.grad_b\n",
    "\n",
    "    '''Fit training data'''\n",
    "    def fit(self,X_train,y_train,X_test,y_test,learning_rate=0.001, beta1=0.9, beta2=0.999, epochs=100, optimizer=None, dropout=1):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X_train: Train data or features\n",
    "        :param y_train: Train targets\n",
    "        :param X_test: Test data or features\n",
    "        :param y_test: Test targets\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param beta1, beta2: Beta values for Momentum / Adam updates\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        :optimizer: Gradient Descent optimizer, can be \"momentum\", \"adam\" or None\n",
    "        :dropout: Dropout fraction between 0 to 1, 1 means no dropout\n",
    "        \"\"\" \n",
    "        # Store dropout fraction\n",
    "        self.dropout_fraction = dropout\n",
    "        # To store test accuracy of each epoch\n",
    "        accuracies = []\n",
    "        # To store max test accuracy, min loss and their epoch\n",
    "        max_accuracy = 0\n",
    "        max_epoch = -1\n",
    "        min_loss = 1e20\n",
    "        min_epoch = -1\n",
    "        # Convert inputs into arrays\n",
    "        X=np.array(X_train)\n",
    "        y=np.array(y_train)\n",
    "        # Array to store the loss of each epoch\n",
    "        to_return = np.zeros(epochs)\n",
    "\n",
    "        '''Train data for each epoch'''\n",
    "        for k in range(epochs):\n",
    "            # Initialise loss for each training sample\n",
    "            loss=np.zeros(X.shape[0])\n",
    "            # For each training sample\n",
    "            for it in range(X.shape[0]):\n",
    "                i=np.random.randint(X.shape[0])\n",
    "                # forward pass\n",
    "                y_hat = self.forward(X[i],False)\n",
    "                # backward pass\n",
    "                loss[it],delta=self.cross_entropy_loss(y[i],y_hat)\n",
    "                self.backward(delta,learning_rate)\n",
    "                # update parameters\n",
    "                self.update(learning_rate,beta1,beta2,optimizer)\n",
    "            # Calculate the mean loss over all training samples\n",
    "            to_return[k] = np.mean(loss) \n",
    "            \n",
    "            '''Calculate and print accuracy, loss in each epoch'''\n",
    "            # Predict on test data\n",
    "            y_pred_test = self.predict(X_test)\n",
    "            y_pred_test = np.argmax(y_pred_test,axis=1)\n",
    "            # Predict on train data\n",
    "            y_pred_train = self.predict(X_train)\n",
    "            y_pred_train = np.argmax(y_pred_train,axis=1)\n",
    "            # Print test/train accuracies and loss\n",
    "            print('Epoch: {}\\t| Test Accuracy: {:0.2f}% | Train Accuracy: {:0.2f}% | Loss: {:0.4f}'.format(k+1,getAccuracy(y_test, y_pred_test)*100,getAccuracy(y_train, y_pred_train)*100,to_return[k]))\n",
    "            # Keep test accuracy \n",
    "            accuracies.append(getAccuracy(y_test, y_pred_test)*100)\n",
    "            # Obtain max accuracy and min loss values\n",
    "            if getAccuracy(y_test, y_pred_test)*100 > max_accuracy:\n",
    "                max_accuracy = getAccuracy(y_test, y_pred_test)*100\n",
    "                max_epoch = k+1\n",
    "            if to_return[k] < min_loss:\n",
    "                min_loss = to_return[k]\n",
    "                min_epoch = k+1\n",
    "            \n",
    "        # Calculate and print mean accuracy, loss over all epochs\n",
    "        print('Mean Accuracy: {:0.2f}%'.format(np.mean(accuracies)))\n",
    "        print('Maximum Accuracy: {:0.2f}% reached at epoch {}'.format(max_accuracy,max_epoch))\n",
    "        print('Mean Loss: {:0.4f}'.format(np.mean(to_return)))\n",
    "        print('Minimum Loss: {:0.4f} reached at epoch {}'.format(min_loss,min_epoch))\n",
    "        \n",
    "        # Return loss values of all epochs\n",
    "        return to_return\n",
    "    \n",
    "\n",
    "    '''Predict output for test data'''\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        output = np.zeros((x.shape[0],self.output_count))\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = nn.forward(x[i,:],True)\n",
    "        return output\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate accuracy between predict and true outputs'''\n",
    "def getAccuracy(y_true, y_pred):\n",
    "    correct = np.sum(y_true == y_pred)\n",
    "    return (correct/float(len(y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\t| Test Accuracy: 84.83% | Train Accuracy: 85.06% | Loss: 0.5350\n",
      "Epoch: 2\t| Test Accuracy: 85.80% | Train Accuracy: 86.53% | Loss: 0.3970\n",
      "Epoch: 3\t| Test Accuracy: 86.53% | Train Accuracy: 87.31% | Loss: 0.3616\n",
      "Epoch: 4\t| Test Accuracy: 87.33% | Train Accuracy: 88.24% | Loss: 0.3446\n",
      "Epoch: 5\t| Test Accuracy: 87.42% | Train Accuracy: 88.64% | Loss: 0.3254\n",
      "Epoch: 6\t| Test Accuracy: 87.48% | Train Accuracy: 88.81% | Loss: 0.3060\n",
      "Epoch: 7\t| Test Accuracy: 87.10% | Train Accuracy: 88.83% | Loss: 0.2998\n",
      "Epoch: 8\t| Test Accuracy: 87.97% | Train Accuracy: 89.67% | Loss: 0.2886\n",
      "Epoch: 9\t| Test Accuracy: 88.02% | Train Accuracy: 89.86% | Loss: 0.2825\n",
      "Epoch: 10\t| Test Accuracy: 88.07% | Train Accuracy: 90.19% | Loss: 0.2751\n",
      "Epoch: 11\t| Test Accuracy: 88.10% | Train Accuracy: 90.17% | Loss: 0.2662\n",
      "Epoch: 12\t| Test Accuracy: 88.28% | Train Accuracy: 90.76% | Loss: 0.2574\n",
      "Epoch: 13\t| Test Accuracy: 88.12% | Train Accuracy: 90.71% | Loss: 0.2521\n",
      "Epoch: 14\t| Test Accuracy: 88.52% | Train Accuracy: 90.91% | Loss: 0.2464\n",
      "Epoch: 15\t| Test Accuracy: 88.05% | Train Accuracy: 91.39% | Loss: 0.2416\n",
      "Epoch: 16\t| Test Accuracy: 88.40% | Train Accuracy: 91.12% | Loss: 0.2401\n",
      "Epoch: 17\t| Test Accuracy: 88.28% | Train Accuracy: 91.45% | Loss: 0.2317\n",
      "Epoch: 18\t| Test Accuracy: 88.50% | Train Accuracy: 91.73% | Loss: 0.2277\n",
      "Epoch: 19\t| Test Accuracy: 88.30% | Train Accuracy: 91.91% | Loss: 0.2211\n",
      "Epoch: 20\t| Test Accuracy: 88.02% | Train Accuracy: 91.99% | Loss: 0.2215\n",
      "Epoch: 21\t| Test Accuracy: 88.47% | Train Accuracy: 91.65% | Loss: 0.2132\n",
      "Epoch: 22\t| Test Accuracy: 88.10% | Train Accuracy: 91.66% | Loss: 0.2149\n",
      "Epoch: 23\t| Test Accuracy: 88.27% | Train Accuracy: 92.24% | Loss: 0.2060\n",
      "Epoch: 24\t| Test Accuracy: 88.00% | Train Accuracy: 92.37% | Loss: 0.2067\n",
      "Epoch: 25\t| Test Accuracy: 88.42% | Train Accuracy: 92.69% | Loss: 0.1994\n",
      "Epoch: 26\t| Test Accuracy: 88.18% | Train Accuracy: 92.65% | Loss: 0.2006\n",
      "Epoch: 27\t| Test Accuracy: 88.42% | Train Accuracy: 92.87% | Loss: 0.1991\n",
      "Epoch: 28\t| Test Accuracy: 88.55% | Train Accuracy: 92.63% | Loss: 0.1907\n",
      "Epoch: 29\t| Test Accuracy: 88.33% | Train Accuracy: 92.62% | Loss: 0.1873\n",
      "Epoch: 30\t| Test Accuracy: 88.30% | Train Accuracy: 92.93% | Loss: 0.1939\n",
      "Epoch: 31\t| Test Accuracy: 88.48% | Train Accuracy: 93.24% | Loss: 0.1840\n",
      "Epoch: 32\t| Test Accuracy: 88.27% | Train Accuracy: 93.44% | Loss: 0.1846\n",
      "Epoch: 33\t| Test Accuracy: 88.58% | Train Accuracy: 93.47% | Loss: 0.1766\n",
      "Epoch: 34\t| Test Accuracy: 87.98% | Train Accuracy: 93.28% | Loss: 0.1775\n",
      "Epoch: 35\t| Test Accuracy: 88.25% | Train Accuracy: 93.89% | Loss: 0.1757\n",
      "Epoch: 36\t| Test Accuracy: 88.07% | Train Accuracy: 93.63% | Loss: 0.1743\n",
      "Epoch: 37\t| Test Accuracy: 88.33% | Train Accuracy: 93.57% | Loss: 0.1698\n",
      "Epoch: 38\t| Test Accuracy: 88.30% | Train Accuracy: 93.51% | Loss: 0.1690\n",
      "Epoch: 39\t| Test Accuracy: 88.22% | Train Accuracy: 94.08% | Loss: 0.1665\n",
      "Epoch: 40\t| Test Accuracy: 88.32% | Train Accuracy: 94.36% | Loss: 0.1652\n",
      "Epoch: 41\t| Test Accuracy: 88.30% | Train Accuracy: 94.11% | Loss: 0.1613\n",
      "Epoch: 42\t| Test Accuracy: 88.53% | Train Accuracy: 94.01% | Loss: 0.1625\n",
      "Epoch: 43\t| Test Accuracy: 88.17% | Train Accuracy: 94.55% | Loss: 0.1596\n",
      "Epoch: 44\t| Test Accuracy: 88.38% | Train Accuracy: 94.42% | Loss: 0.1560\n",
      "Epoch: 45\t| Test Accuracy: 88.32% | Train Accuracy: 94.61% | Loss: 0.1574\n",
      "Epoch: 46\t| Test Accuracy: 88.10% | Train Accuracy: 94.57% | Loss: 0.1570\n",
      "Epoch: 47\t| Test Accuracy: 88.20% | Train Accuracy: 94.46% | Loss: 0.1522\n",
      "Epoch: 48\t| Test Accuracy: 88.20% | Train Accuracy: 94.82% | Loss: 0.1559\n",
      "Epoch: 49\t| Test Accuracy: 88.07% | Train Accuracy: 94.89% | Loss: 0.1445\n",
      "Epoch: 50\t| Test Accuracy: 88.13% | Train Accuracy: 94.87% | Loss: 0.1451\n",
      "Mean Accuracy: 88.03%\n",
      "Maximum Accuracy: 88.58% reached at epoch 33\n",
      "Mean Loss: 0.2226\n",
      "Minimum Loss: 0.1445 reached at epoch 49\n",
      "Wall time: 28min 4s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22d28034ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "'''Define training and test data of ratio 9:1'''\n",
    "# Test\n",
    "x_train = data[0:54000,:]\n",
    "y_train = label[0:54000]\n",
    "# Train\n",
    "x_test = data[54000:60000,:]\n",
    "y_test = label[54000:60000]\n",
    "\n",
    "'''Initialise the proposed neural network'''\n",
    "nn = MLP([128,100,100,10],['tanh','relu','softmax'])\n",
    "\n",
    "'''Fit data to train the model'''\n",
    "loss = nn.fit(x_train, y_train, x_test, y_test, learning_rate=0.001, beta1=0.9, beta2=0.999, epochs=50, optimizer='momentum', dropout=1) \n",
    "\n",
    "'''Plot loss'''\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(loss)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(10000,)\n",
      "[9 2 1 1 0 1 4 6 5 7 4 5 5 3 4 1 2 2 8 0 2 5 7 5 1 4 4 0 9 4 8 8 3 3 8 0 7\n",
      " 5 7 9 0 1 6 5 4 9 2 1 2 6 4 4 5 0 2 2 8 4 8 0 7 7 8 5 1 1 0 4 7 8 7 0 6 6\n",
      " 2 1 1 2 8 4 1 8 5 9 5 0 1 2 0 0 5 1 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "'''Check predicted outputs for test data from Predicted_labels.h5'''\n",
    "\n",
    "# Print its type, shape, first 100 predicted labels\n",
    "print(type(predicted_label))\n",
    "print(predicted_label.shape)\n",
    "print(predicted_label[0:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
