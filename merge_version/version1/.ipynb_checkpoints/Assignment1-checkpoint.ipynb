{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OigpLIHWTQU5"
   },
   "source": [
    "# COMP5329 - Deep Learning\n",
    "## Assignment 1 - Multilayer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S9Dx7C_TQU6"
   },
   "source": [
    "**Semester 1, 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxMRrQEBTQU7"
   },
   "source": [
    "## Loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "PiIktNG2TQU8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as pl\n",
    "from ipywidgets import interact, widgets\n",
    "from matplotlib import animation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UfknvYWYTQU_"
   },
   "source": [
    " ## Load the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "IU9g6--zTQU_",
    "outputId": "02ff4f5a-bda0-4e3a-eb9f-e9e15e7b2f02"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "X_train = np.load(\"../raw_data/train_data.npy\")\n",
    "X_test = np.load(\"../raw_data/test_data.npy\")\n",
    "y_train = np.load(\"../raw_data/train_label.npy\")\n",
    "y_test = np.load(\"../raw_data/test_label.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1YLqe0zTQVF"
   },
   "source": [
    "## Show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "id": "oSvKdlYXAFTZ",
    "outputId": "144fc103-4d1b-4d99-87f5-458afe31239a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 128)\n",
      "(10000, 128)\n",
      "(50000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.asdtype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBbVI6fyTQVI"
   },
   "source": [
    "## Definition of some activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4ZooR-OTQVK"
   },
   "source": [
    "Linear\n",
    "$$output = x$$\n",
    "\n",
    "Tanh  \n",
    "$$output = tanh(x)$$  \n",
    "\n",
    "Sigmoid\n",
    "$$output = \\frac {1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "csIF2JfSTQVK"
   },
   "outputs": [],
   "source": [
    "# create a activation class\n",
    "# for each time, we can initiale a activation function object with one specific function\n",
    "# for example: f = Activation(\"tanh\")  means we create a tanh activation function.\n",
    "# you can define more activation functions by yourself, such as relu!\n",
    "\n",
    "\n",
    "class Activation(object):\n",
    "    def __tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    def __relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    def __relu_deriv(self,a):     \n",
    "        return(np.where(a>0,1,0))     \n",
    "        \n",
    "    def __tanh_deriv(self, a):\n",
    "        # a = np.tanh(x)   \n",
    "        return 1.0 - a**2\n",
    "    def __logistic(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def __logistic_deriv(self, a):\n",
    "        # a = logistic(x) \n",
    "        return  a * (1 - a)\n",
    "    \n",
    "    def __softmax(self,a):\n",
    "        shift = a - np.max(a)\n",
    "        return np.exp(shift) / np.sum(np.exp(shift))\n",
    "        \n",
    "    def __softmax_deriv(self, a):\n",
    "        case1 = -np.dot(a.T,a)\n",
    "        case2 = np.dot(a,1-a)\n",
    "        np.fill_diagonal(case1,np.diag(a*1-a))\n",
    "        return(output)\n",
    "    \n",
    "    def __init__(self,activation='tanh'):\n",
    "        if activation == 'logistic':\n",
    "            self.f = self.__logistic\n",
    "            self.f_deriv = self.__logistic_deriv\n",
    "        elif activation == \"softmax\":\n",
    "            self.f = self.__softmax\n",
    "            self.f_deriv = self.__softmax_deriv\n",
    "        elif activation == 'tanh':\n",
    "            self.f = self.__tanh\n",
    "            self.f_deriv = self.__tanh_deriv\n",
    "        elif activation == 'ReLU':\n",
    "            self.f = self.__relu\n",
    "            self.f_deriv = self.__relu_deriv\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91dBdVq4TQVN"
   },
   "source": [
    "### Define HiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBnDirODTQVO"
   },
   "source": [
    "$$output = f\\_act(\\sum_{i=0}^{1}{(I_{i} * W_{i})} + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "GymZcsO0TQVO"
   },
   "outputs": [],
   "source": [
    "# now we define the hidden layer for the mlp\n",
    "# for example, h1 = HiddenLayer(10, 5, activation=\"tanh\") means we create a layer with 10 dimension input and 5 dimension output, and using tanh activation function.\n",
    "# notes: make sure the input size of hiddle layer should be matched with the output size of the previous layer!\n",
    "\n",
    "class HiddenLayer(object):    \n",
    "    def __init__(self,n_in, n_out,dropout = 0.5,\n",
    "                 activation_last_layer='tanh',activation='tanh', W=None, b=None):\n",
    "        \"\"\"\n",
    "        Typical hidden layer of a MLP: units are fully-connected and have\n",
    "        sigmoidal activation function. Weight matrix W is of shape (n_in,n_out)\n",
    "        and the bias vector b is of shape (n_out,).\n",
    "\n",
    "        NOTE : The nonlinearity used here is tanh\n",
    "\n",
    "        Hidden unit activation is given by: tanh(dot(input,W) + b)\n",
    "\n",
    "        :type n_in: int\n",
    "        :param n_in: dimensionality of input\n",
    "\n",
    "        :type n_out: int\n",
    "        :param n_out: number of hidden units\n",
    "\n",
    "        :type activation: string\n",
    "        :param activation: Non linearity to be applied in the hidden\n",
    "                           layer\n",
    "        \"\"\"\n",
    "        self.input=None\n",
    "        self.activation=Activation(activation).f\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # activation deriv of last layer\n",
    "        self.activation_deriv=None\n",
    "        if activation_last_layer:\n",
    "            self.activation_deriv=Activation(activation_last_layer).f_deriv\n",
    "\n",
    "        # we randomly assign small values for the weights as the initiallization\n",
    "        self.W = np.random.uniform(\n",
    "                low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                high=np.sqrt(6. / (n_in + n_out)),\n",
    "                size=(n_in, n_out)\n",
    "        )\n",
    "        # if activation == 'logistic':\n",
    "        #     self.W *= 4\n",
    "\n",
    "        # we set the size of bias as the size of output dimension\n",
    "        self.b = np.zeros(n_out,)\n",
    "        \n",
    "        # we set he size of weight gradation as the size of weight\n",
    "        self.grad_W = np.zeros(self.W.shape)\n",
    "        self.grad_b = np.zeros(self.b.shape)\n",
    "        \n",
    "        # set velocity term \n",
    "        self.v_W = np.zeros(self.W.shape)\n",
    "        self.v_b = np.zeros(self.b.shape)        \n",
    "        \n",
    "    \n",
    "    # the forward and backward progress (in the hidden layer level) for each training epoch\n",
    "    # please learn the week2 lec contents carefully to understand these codes. \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        :type input: numpy.array\n",
    "        :param input: a symbolic tensor of shape (n_in,)\n",
    "        '''\n",
    "        lin_output = np.dot(input, self.W) + self.b\n",
    "        self.output = (\n",
    "            lin_output if self.activation is None\n",
    "            else self.activation(lin_output)\n",
    "        )\n",
    "        self.input=input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, delta, output_layer=False):         \n",
    "        self.grad_W = np.atleast_2d(self.input).T.dot(np.atleast_2d(delta))\n",
    "        self.grad_b = delta\n",
    "        if self.activation_deriv:\n",
    "            delta = delta.dot(self.W.T) * self.activation_deriv(self.input)\n",
    "        return delta\n",
    "    \n",
    "    \n",
    "#     def dropout_forward(self, input):\n",
    "#         self.mask = np.random.choice([0, 1], size=input.shape, p=[self.dropoutrate, 1 - self.dropoutrate])\n",
    "#         input *= self.mask       \n",
    "        \n",
    "        \n",
    "#     def dropout_backward(self, delta, output_layer=False):   \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwsUH3fhTQVR"
   },
   "source": [
    "## The MLP\n",
    "\n",
    "The class implements a MLP with a fully configurable number of layers and neurons. It adapts its weights using the backpropagation algorithm in an online manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "id": "QWpvH41iTQVR"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    \"\"\" \n",
    "    # for initiallization, the code will create all layers automatically based on the provided parameters.     \n",
    "    def __init__(self, layers, optimiser, weight_decay = 1, activation=[None,'tanh','tanh']):\n",
    "        \"\"\"\n",
    "        :param layers: A list containing the number of units in each layer.\n",
    "        Should be at least two values\n",
    "        :param activation: The activation function to be used. Can be\n",
    "        \"logistic\" or \"tanh\"\n",
    "        \"\"\"        \n",
    "        ### initialize layers \n",
    "        self.layers=[]\n",
    "        self.params=[]\n",
    "        \n",
    "        self.activation=activation\n",
    "        self.optimiser = optimiser\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.layers.append(HiddenLayer(layers[i],layers[i+1],activation[i],activation[i+1]))\n",
    "\n",
    "    # forward progress: pass the information through the layers and out the results of final output layer\n",
    "    def forward(self,input):\n",
    "        for layer in self.layers:\n",
    "            output=layer.forward(input)\n",
    "            input=output\n",
    "            \n",
    "#             print(input.shape)\n",
    "#             print(output.shape)\n",
    "        return output\n",
    "\n",
    "    # define the objection/loss function, we use mean sqaure error (MSE) as the loss\n",
    "    # you can try other loss, such as cross entropy.\n",
    "    # when you try to change the loss, you should also consider the backward formula for the new loss as well!\n",
    "    def criterion_MSE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        # MSE\n",
    "        error = y-y_hat\n",
    "        loss=np.sum(error**2)\n",
    "        # calculate the MSE's delta of the output layer\n",
    "        delta=-error*activation_deriv(y_hat)    \n",
    "        # return loss and delta\n",
    "        \n",
    "        return loss,delta\n",
    "    \n",
    "    # Cross entropy loss\n",
    "    def criterion_CE(self,y,y_hat):\n",
    "        activation_deriv=Activation(self.activation[-1]).f_deriv\n",
    "        loss = -np.mean(y * np.log(y_hat+ 1e-6))\n",
    "        delta =  -(y - y_hat) / y.shape[0]\n",
    "        \n",
    "        return loss,delta       \n",
    "\n",
    "    # backward progress  \n",
    "    def backward(self,delta):\n",
    "        delta=self.layers[-1].backward(delta,output_layer=True)\n",
    "        for layer in reversed(self.layers[:-1]):\n",
    "            delta=layer.backward(delta)\n",
    "\n",
    "    # update the network weights after backward.\n",
    "    # make sure you run the backward function before the update function!    \n",
    "    def update(self,lr):\n",
    "        if self.optimiser[0] == \"SGD\":\n",
    "            for layer in self.layers:\n",
    "                layer.W -= lr * layer.grad_W\n",
    "                layer.b -= lr * layer.grad_b*self.weight_decay\n",
    "                layer.W *= self.weight_decay\n",
    "                layer.b *= self.weight_decay\n",
    "        elif self.optimiser[0] == \"SGD_momentum\":\n",
    "             for layer in self.layers:\n",
    "                    layer.v_W = (optimiser[1] * layer.v_W) + (lr * layer.grad_W)\n",
    "                    layer.v_b = (optimiser[1] * layer.v_b) + (lr * layer.grad_b)\n",
    "                    layer.W = layer.W - layer.v_W\n",
    "                    layer.b = layer.b - layer.v_b\n",
    "\n",
    "                    layer.W *= self.weight_decay\n",
    "                    layer.b *= self.weight_decay\n",
    "        else:\n",
    "            # Adam\n",
    "            for layer in self.layers:\n",
    "                layer.W -= lr * layer.grad_W\n",
    "                layer.b -= lr * layer.grad_b\n",
    "                layer.W *= self.weight_decay\n",
    "                layer.b *= self.weight_decay\n",
    "\n",
    "    # define the training function\n",
    "    # it will return all losses within the whole training process.\n",
    "    def fit(self,X,y,optimiser, mini_batch_size = 10, learning_rate=0.1, epochs=100):\n",
    "        \"\"\"\n",
    "        Online learning.\n",
    "        :param X: Input data or features\n",
    "        :param y: Input targets\n",
    "        :param: optimiser: a list contain [0]: optimiser type, [1]: size of beta1, [2]: size of beta2\n",
    "        :param learning_rate: parameters defining the speed of learning\n",
    "        :param epochs: number of times the dataset is presented to the network for learning\n",
    "        :param mini_batch_size: indicate each batch size\n",
    "        \"\"\" \n",
    "        X=np.array(X)\n",
    "        y=np.array(y)\n",
    "        to_return = np.zeros(epochs)\n",
    "        batches_num = int(np.ceil(X.shape[0] / mini_batch_size))\n",
    "        \n",
    "        \n",
    "        for k in range(epochs):\n",
    "            loss=np.zeros(mini_batch_size)\n",
    "            current_index= 0\n",
    "            X,y = shuffle_randomly(X, y)\n",
    "            for it in range(batches_num):\n",
    "                \n",
    "                # Case 1 if the last index is not larger than datasize\n",
    "                if( (current_index + mini_batch_size) <= X.shape[0]):\n",
    "                    X_curr = X[current_index:current_index + mini_batch_size]\n",
    "                    y_curr = y[current_index:current_index + mini_batch_size]\n",
    "                else:\n",
    "                # Case 2: reach end of the data\n",
    "                    X_curr = X[current_index:]\n",
    "                    y_curr = y[current_index:]   \n",
    "                \n",
    "                \n",
    "                # forward pass\n",
    "                y_hat = self.forward(X_curr)               \n",
    "                \n",
    "                # backward pass\n",
    "                loss[it],delta=self.criterion_CE(y_curr,y_hat)\n",
    "                self.backward(delta)\n",
    "\n",
    "                # update\n",
    "                self.update(learning_rate)\n",
    "                \n",
    "                # Add mini-batch-size\n",
    "                current_index += mini_batch_size\n",
    "                   \n",
    "            to_return[k] = np.mean(loss)\n",
    "        return to_return\n",
    "\n",
    "    # define the prediction function\n",
    "    # we can use predict function to predict the results of new data, by using the well-trained network.\n",
    "    def predict(self, x):\n",
    "        x = np.array(x)\n",
    "        output = np.zeros(x.shape[0])\n",
    "        for i in np.arange(x.shape[0]):\n",
    "            output[i] = self.forward(x[i,:])\n",
    "        return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_randomly(X, y):\n",
    "    randomize = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    return X[randomize], y[randomize]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KlgcO0CNTQVU"
   },
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "id": "iu7RlvZYTQVV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SGD_momentum', 0.9]\n"
     ]
    }
   ],
   "source": [
    "### Try different MLP models\n",
    "SGD_optimiser = ['SGD_momentum',0.9]\n",
    "nn = MLP([128, 6, 10], SGD_optimiser,1, [None,'ReLU','softmax'])\n",
    "print(nn.optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEo7LPtITQVX",
    "outputId": "6bede9b3-b5f9-4612-fc18-0ad20633c33c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.91440872  0.89886199 -0.65486877 -0.57482779  0.27308586 -1.04767476]\n",
      " [ 0.89886199 -2.22883767  0.4594266   1.22414782 -2.50043324  1.29944185]\n",
      " [-0.65486877  0.4594266  -3.00766779 -3.14461555  0.26372559 -2.63078628]\n",
      " [-0.57482779  1.22414782 -3.14461555 -4.52089346  1.0260325  -3.80315366]\n",
      " [ 0.27308586 -2.50043324  0.26372559  1.0260325  -3.24589274  0.96665797]\n",
      " [-1.04767476  1.29944185 -2.63078628 -3.80315366  0.96665797 -3.61454321]]\n",
      "[[ 0.          0.89886199 -0.65486877 -0.57482779  0.27308586 -1.04767476]\n",
      " [ 0.89886199  0.          0.4594266   1.22414782 -2.50043324  1.29944185]\n",
      " [-0.65486877  0.4594266   0.         -3.14461555  0.26372559 -2.63078628]\n",
      " [-0.57482779  1.22414782 -3.14461555  0.          1.0260325  -3.80315366]\n",
      " [ 0.27308586 -2.50043324  0.26372559  1.0260325   0.          0.96665797]\n",
      " [-1.04767476  1.29944185 -2.63078628 -3.80315366  0.96665797  0.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12958\\AppData\\Local\\Temp\\ipykernel_27904\\3482223220.py:50: RuntimeWarning: invalid value encountered in log\n",
      "  loss = -np.mean(y * np.log(y_hat+ 1e-6))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,6) (6,6) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27904\\1956417105.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m### Try different learning rate and epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mMSE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss:%f'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mMSE\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27904\\3482223220.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, optimiser, mini_batch_size, learning_rate, epochs)\u001b[0m\n\u001b[0;32m    124\u001b[0m                 \u001b[1;31m# backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion_CE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_curr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;31m# update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27904\\3482223220.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[1;31m# backward progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mdelta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27904\\3998867755.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, delta, output_layer)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_deriv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_deriv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdelta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,6) (6,6) "
     ]
    }
   ],
   "source": [
    "### Try different learning rate and epochs\n",
    "\n",
    "MSE = nn.fit(X_train[:10], y_train[:10], optimiser, mini_batch_size = 5, learning_rate=0.001, epochs=500)\n",
    "print('loss:%f'%MSE[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T05l4p9mTQVZ"
   },
   "source": [
    "#### Plot loss in epochs\n",
    "We can visualize the loss change during the training process, to under how we can the network. As we can see, the loss staies at the large level at the beginning, but drop quickly within the training. A small loss value indicate a well-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "gFAUpnzyTQVa",
    "outputId": "cc83ad29-cba2-4272-ec27-bdd8e300af23"
   },
   "outputs": [],
   "source": [
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "vLGdal7hQUEa",
    "outputId": "7c57e6d3-6a6d-470f-b716-a4c4abcbf2d0"
   },
   "outputs": [],
   "source": [
    "### Try different MLP models\n",
    "# we can compare the loss change graph to under how the network parameters (such as number of layers and activation functions),\n",
    "# could affect the performance of network.\n",
    "nn = MLP([2,3,1], [None,'logistic','tanh'])\n",
    "input_data = dataset[:,0:2]\n",
    "output_data = dataset[:,2]\n",
    "MSE = nn.fit(input_data, output_data, learning_rate=0.0001, epochs=500)\n",
    "print('loss:%f'%MSE[-1])\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Yu8RX72IQe7Q",
    "outputId": "66d4f1f2-83e7-4521-ad23-0ab4466e9029"
   },
   "outputs": [],
   "source": [
    "### Try different MLP models\n",
    "nn = MLP([2,3,1], [None,'logistic','tanh'])\n",
    "input_data = dataset[:,0:2]\n",
    "output_data = dataset[:,2]\n",
    "MSE = nn.fit(input_data, output_data, learning_rate=0.1, epochs=500)\n",
    "print('loss:%f'%MSE[-1])\n",
    "pl.figure(figsize=(15,4))\n",
    "pl.plot(MSE)\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gN93ZYW3TQVc"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "XpAUm5T7TQVd"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27904\\2099685097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "output = nn.predict(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "Y9DTHUbXTQVg",
    "outputId": "656b092f-be26-4f6d-90ab-1f97e642f488"
   },
   "outputs": [],
   "source": [
    "# visualizing the predict results\n",
    "# notes: since we use tanh function for the final layer, that means the output will be in range of [0,1]\n",
    "pl.figure(figsize=(8,6))\n",
    "pl.scatter(output_data, output, s=100)\n",
    "pl.xlabel('Targets')\n",
    "pl.ylabel('MLP output')\n",
    "pl.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "3OR0tt8fTQVi",
    "outputId": "8884511b-6f57-4177-d6a9-4f41e1de5cf8"
   },
   "outputs": [],
   "source": [
    "# create a mesh to plot in\n",
    "xx, yy = np.meshgrid(np.arange(-2, 2, .02),np.arange(-2, 2, .02))\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "pl.figure(figsize=(15,7))\n",
    "pl.subplot(1,2,1)\n",
    "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output_data], s=100)\n",
    "pl.xlim(-2, 2)\n",
    "pl.ylim(-2, 2)\n",
    "pl.grid()\n",
    "pl.title('Targets')\n",
    "pl.subplot(1,2,2)\n",
    "pl.pcolormesh(xx, yy, Z>0, cmap='cool')\n",
    "pl.scatter(input_data[:,0], input_data[:,1], c=[(['b', 'r'])[d>0] for d in output], s=100)\n",
    "pl.xlim(-2, 2)\n",
    "pl.ylim(-2, 2)\n",
    "pl.grid()\n",
    "pl.title('MLP output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytqTFdkQfobY"
   },
   "source": [
    "### the figure on the left shows the ground true label of each data\n",
    "### the figure on the eright shows the predict label of each data with MLP model.\n",
    "### Based on the visualization result, we can find that network learned a boundary between positive and negative data!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
